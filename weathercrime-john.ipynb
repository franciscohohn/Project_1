{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies and Setup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import datetime as dt\n",
    "import json\n",
    "import csv\n",
    "import pytemperature as pyt\n",
    "from pprint import pprint\n",
    "\n",
    "# Import API key\n",
    "from api_keys import api_key\n",
    "\n",
    "# files to Load\n",
    "cpd_crime2018_load = \"Resources/Crimes_-_2018.csv\"\n",
    "cpd_crime2017_load = \"Resources/Crimes_-_2017.csv\"\n",
    "cpd_crime2016_load = \"Resources/Crimes_-_2016.csv\"\n",
    "cpd_crime2015_load = \"Resources/Crimes_-_2015.csv\"\n",
    "cpd_crime2014_load = \"Resources/Crimes_-_2014.csv\"\n",
    "\n",
    "# read CPD crime data and store\n",
    "cpdcrime2018_data = pd.read_csv(cpd_crime2018_load)\n",
    "cpdcrime2017_data = pd.read_csv(cpd_crime2017_load)\n",
    "cpdcrime2016_data = pd.read_csv(cpd_crime2016_load)\n",
    "cpdcrime2015_data = pd.read_csv(cpd_crime2015_load)\n",
    "cpdcrime2014_data = pd.read_csv(cpd_crime2014_load)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull column data and drop NaN \n",
    "cpdcrime2018_data = cpdcrime2018_data[['Date', 'Primary Type']]\n",
    "cpdcrime2018_data = cpdcrime2018_data.dropna(how='any')\n",
    "\n",
    "cpdcrime2017_data = cpdcrime2017_data[['Date', 'Primary Type']]\n",
    "cpdcrime2017_data = cpdcrime2017_data.dropna(how='any')\n",
    "\n",
    "cpdcrime2016_data = cpdcrime2016_data[['Date', 'Primary Type']]\n",
    "cpdcrime2016_data = cpdcrime2016_data.dropna(how='any')\n",
    "\n",
    "cpdcrime2015_data = cpdcrime2015_data[['Date', 'Primary Type']]\n",
    "cpdcrime2015_data = cpdcrime2015_data.dropna(how='any')\n",
    "\n",
    "cpdcrime2014_data = cpdcrime2014_data[['Date', 'Primary Type']]\n",
    "cpdcrime2014_data = cpdcrime2014_data.dropna(how='any')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use to check outputs\n",
    "#cpdcrime2018_data.head()\n",
    "#cpdcrime2017_data.head()\n",
    "#cpdcrime2016_data.head()\n",
    "#cpdcrime2015_data.head()\n",
    "#cpdcrime2014_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create series with the date of each reported incident by primary type\n",
    "battery18 = cpdcrime2018_data[cpdcrime2018_data['Primary Type'] == \"BATTERY\"][\"Date\"].reset_index()\n",
    "battery17 = cpdcrime2017_data[cpdcrime2017_data['Primary Type'] == \"BATTERY\"][\"Date\"].reset_index()\n",
    "battery16 = cpdcrime2016_data[cpdcrime2016_data['Primary Type'] == \"BATTERY\"][\"Date\"].reset_index()\n",
    "battery15 = cpdcrime2015_data[cpdcrime2015_data['Primary Type'] == \"BATTERY\"][\"Date\"].reset_index()\n",
    "battery14 = cpdcrime2014_data[cpdcrime2014_data['Primary Type'] == \"BATTERY\"][\"Date\"].reset_index()\n",
    "\n",
    "#format date to be mergeable with weather data \n",
    "battery18['Date'] = pd.to_datetime(battery18['Date']).dt.date\n",
    "battery18['Date'] = pd.to_datetime(battery18['Date'], infer_datetime_format=True)\n",
    "battery17['Date'] = pd.to_datetime(battery17['Date']).dt.date\n",
    "battery17['Date'] = pd.to_datetime(battery17['Date'], infer_datetime_format=True)\n",
    "battery16['Date'] = pd.to_datetime(battery16['Date']).dt.date\n",
    "battery16['Date'] = pd.to_datetime(battery16['Date'], infer_datetime_format=True)\n",
    "battery15['Date'] = pd.to_datetime(battery15['Date']).dt.date\n",
    "battery15['Date'] = pd.to_datetime(battery15['Date'], infer_datetime_format=True)\n",
    "battery14['Date'] = pd.to_datetime(battery14['Date']).dt.date\n",
    "battery14['Date'] = pd.to_datetime(battery14['Date'], infer_datetime_format=True)\n",
    "\n",
    "#sort date data for chronological order\n",
    "battery18.sort_values(\"Date\", inplace=True)\n",
    "battery17.sort_values(\"Date\", inplace=True)\n",
    "battery16.sort_values(\"Date\", inplace=True)\n",
    "battery15.sort_values(\"Date\", inplace=True)\n",
    "battery14.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "#create number of reported incidents by date \n",
    "battery18['Reported Incidents'] = battery18['Date'].map(battery18['Date'].value_counts())\n",
    "battery17['Reported Incidents'] = battery17['Date'].map(battery17['Date'].value_counts())\n",
    "battery16['Reported Incidents'] = battery16['Date'].map(battery16['Date'].value_counts())\n",
    "battery15['Reported Incidents'] = battery15['Date'].map(battery15['Date'].value_counts())\n",
    "battery14['Reported Incidents'] = battery14['Date'].map(battery14['Date'].value_counts())\n",
    "\n",
    "#set index to date and drop duplicate data\n",
    "battery18.set_index('Date')\n",
    "battery18.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "battery17.set_index('Date')\n",
    "battery17.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "battery16.set_index('Date')\n",
    "battery16.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "battery15.set_index('Date')\n",
    "battery15.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "battery14.set_index('Date')\n",
    "battery14.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "\n",
    "#delete unnecessary index column\n",
    "del battery18['index']\n",
    "del battery17['index']\n",
    "del battery16['index']\n",
    "del battery15['index']\n",
    "del battery14['index']\n",
    "\n",
    "#merge all reported battery incidents for all five years\n",
    "battery_all = battery18.append([battery17, battery16, battery15, battery14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use to check output\n",
    "#battery18\n",
    "#battery18.dtypes\n",
    "#battery17\n",
    "#battery17.dtypes\n",
    "#battery16\n",
    "#battery16.dtypes\n",
    "#battery15\n",
    "#battery15.dtypes\n",
    "#battery14\n",
    "#battery14.dtypes\n",
    "#battery_all\n",
    "#battery_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create series with the date of each reported incident by primary type\n",
    "csa18 = cpdcrime2018_data[cpdcrime2018_data['Primary Type'] == \"CRIM SEXUAL ASSAULT\"][\"Date\"].reset_index()\n",
    "csa17 = cpdcrime2017_data[cpdcrime2017_data['Primary Type'] == \"CRIM SEXUAL ASSAULT\"][\"Date\"].reset_index()\n",
    "csa16 = cpdcrime2016_data[cpdcrime2016_data['Primary Type'] == \"CRIM SEXUAL ASSAULT\"][\"Date\"].reset_index()\n",
    "csa15 = cpdcrime2015_data[cpdcrime2015_data['Primary Type'] == \"CRIM SEXUAL ASSAULT\"][\"Date\"].reset_index()\n",
    "csa14 = cpdcrime2014_data[cpdcrime2014_data['Primary Type'] == \"CRIM SEXUAL ASSAULT\"][\"Date\"].reset_index()\n",
    "\n",
    "#format date in dataframe to be mergeable with weather data \n",
    "csa18['Date'] = pd.to_datetime(csa18['Date']).dt.date\n",
    "csa18['Date'] = pd.to_datetime(csa18['Date'], infer_datetime_format=True)\n",
    "csa17['Date'] = pd.to_datetime(csa17['Date']).dt.date\n",
    "csa17['Date'] = pd.to_datetime(csa17['Date'], infer_datetime_format=True)\n",
    "csa16['Date'] = pd.to_datetime(csa16['Date']).dt.date\n",
    "csa16['Date'] = pd.to_datetime(csa16['Date'], infer_datetime_format=True)\n",
    "csa15['Date'] = pd.to_datetime(csa15['Date']).dt.date\n",
    "csa15['Date'] = pd.to_datetime(csa15['Date'], infer_datetime_format=True)\n",
    "csa14['Date'] = pd.to_datetime(csa14['Date']).dt.date\n",
    "csa14['Date'] = pd.to_datetime(csa14['Date'], infer_datetime_format=True)\n",
    "\n",
    "#sort date data for chronological order\n",
    "csa18.sort_values(\"Date\", inplace=True)\n",
    "csa17.sort_values(\"Date\", inplace=True)\n",
    "csa16.sort_values(\"Date\", inplace=True)\n",
    "csa15.sort_values(\"Date\", inplace=True)\n",
    "csa14.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "#create number of reported incidents by date \n",
    "csa18['Reported Incidents'] = csa18['Date'].map(csa18['Date'].value_counts())\n",
    "csa17['Reported Incidents'] = csa17['Date'].map(csa17['Date'].value_counts())\n",
    "csa16['Reported Incidents'] = csa16['Date'].map(csa16['Date'].value_counts())\n",
    "csa15['Reported Incidents'] = csa15['Date'].map(csa15['Date'].value_counts())\n",
    "csa14['Reported Incidents'] = csa14['Date'].map(csa14['Date'].value_counts())\n",
    "\n",
    "#set index to date and drop duplicate data\n",
    "csa18.set_index('Date')\n",
    "csa18.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "csa17.set_index('Date')\n",
    "csa17.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "csa16.set_index('Date')\n",
    "csa16.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "csa15.set_index('Date')\n",
    "csa15.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "csa14.set_index('Date')\n",
    "csa14.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "\n",
    "\n",
    "#delete unnecessary index column\n",
    "del csa18['index']\n",
    "del csa17['index']\n",
    "del csa16['index']\n",
    "del csa15['index']\n",
    "del csa14['index']\n",
    "\n",
    "#merge all reported criminal sexual assault incidents for all five years\n",
    "csa_all = csa18.append([csa17, csa16, csa15, csa14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use to check output\n",
    "#csa18\n",
    "#csa18.dtypes\n",
    "#csa17\n",
    "#csa17.dtypes\n",
    "#csa16\n",
    "#csa16.dtypes\n",
    "#csa15\n",
    "#csa15.dtypes\n",
    "#csa14\n",
    "#csa14.dtypes\n",
    "#csa_all\n",
    "#csa_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create series with the date of each reported incident by primary type\n",
    "homicide18 = cpdcrime2018_data[cpdcrime2018_data['Primary Type'] == \"HOMICIDE\"][\"Date\"].reset_index()\n",
    "homicide17 = cpdcrime2017_data[cpdcrime2017_data['Primary Type'] == \"HOMICIDE\"][\"Date\"].reset_index()\n",
    "homicide16 = cpdcrime2016_data[cpdcrime2016_data['Primary Type'] == \"HOMICIDE\"][\"Date\"].reset_index()\n",
    "homicide15 = cpdcrime2015_data[cpdcrime2015_data['Primary Type'] == \"HOMICIDE\"][\"Date\"].reset_index()\n",
    "homicide14 = cpdcrime2014_data[cpdcrime2014_data['Primary Type'] == \"HOMICIDE\"][\"Date\"].reset_index()\n",
    "\n",
    "#format date in dataframe to be mergeable with weather data \n",
    "homicide18['Date'] = pd.to_datetime(homicide18['Date']).dt.date\n",
    "homicide18['Date'] = pd.to_datetime(homicide18['Date'], infer_datetime_format=True)\n",
    "homicide17['Date'] = pd.to_datetime(homicide17['Date']).dt.date\n",
    "homicide17['Date'] = pd.to_datetime(homicide17['Date'], infer_datetime_format=True)\n",
    "homicide16['Date'] = pd.to_datetime(homicide16['Date']).dt.date\n",
    "homicide16['Date'] = pd.to_datetime(homicide16['Date'], infer_datetime_format=True)\n",
    "homicide15['Date'] = pd.to_datetime(homicide15['Date']).dt.date\n",
    "homicide15['Date'] = pd.to_datetime(homicide15['Date'], infer_datetime_format=True)\n",
    "homicide14['Date'] = pd.to_datetime(homicide14['Date']).dt.date\n",
    "homicide14['Date'] = pd.to_datetime(homicide14['Date'], infer_datetime_format=True)\n",
    "\n",
    "#sort date data for chronological order\n",
    "homicide18.sort_values(\"Date\", inplace=True)\n",
    "homicide17.sort_values(\"Date\", inplace=True)\n",
    "homicide16.sort_values(\"Date\", inplace=True)\n",
    "homicide15.sort_values(\"Date\", inplace=True)\n",
    "homicide14.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "#create number of reported incidents by date \n",
    "homicide18['Reported Incidents'] = homicide18['Date'].map(homicide18['Date'].value_counts())\n",
    "homicide17['Reported Incidents'] = homicide17['Date'].map(homicide17['Date'].value_counts())\n",
    "homicide16['Reported Incidents'] = homicide16['Date'].map(homicide16['Date'].value_counts())\n",
    "homicide15['Reported Incidents'] = homicide15['Date'].map(homicide15['Date'].value_counts())\n",
    "homicide14['Reported Incidents'] = homicide14['Date'].map(homicide14['Date'].value_counts())\n",
    "\n",
    "#set index to date and drop duplicate data\n",
    "homicide18.set_index('Date')\n",
    "homicide18.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "homicide17.set_index('Date')\n",
    "homicide17.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "homicide16.set_index('Date')\n",
    "homicide16.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "homicide15.set_index('Date')\n",
    "homicide15.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "homicide14.set_index('Date')\n",
    "homicide14.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "\n",
    "\n",
    "#delete unnecessary index column\n",
    "del homicide18['index']\n",
    "del homicide17['index']\n",
    "del homicide16['index']\n",
    "del homicide15['index']\n",
    "del homicide14['index']\n",
    "\n",
    "\n",
    "#merge all reported homicide incidents for all five years\n",
    "homicide_all = homicide18.append([homicide17, homicide16, homicide15, homicide14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use to check output\n",
    "#homicide18\n",
    "#homicide18.dtypes\n",
    "#homicide17\n",
    "#homicide17.dtypes\n",
    "#homicide16\n",
    "#homicide16.dtypes\n",
    "#homicide15\n",
    "#homicide15.dtypes\n",
    "#homicide14\n",
    "#homicide14.dtypes\n",
    "#homicide_all\n",
    "#homicide_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create series with the date of each reported incident by primary type\n",
    "robbery18 = cpdcrime2018_data[cpdcrime2018_data['Primary Type'] == \"ROBBERY\"][\"Date\"].reset_index()\n",
    "robbery17 = cpdcrime2017_data[cpdcrime2017_data['Primary Type'] == \"ROBBERY\"][\"Date\"].reset_index()\n",
    "robbery16 = cpdcrime2016_data[cpdcrime2016_data['Primary Type'] == \"ROBBERY\"][\"Date\"].reset_index()\n",
    "robbery15 = cpdcrime2015_data[cpdcrime2015_data['Primary Type'] == \"ROBBERY\"][\"Date\"].reset_index()\n",
    "robbery14 = cpdcrime2014_data[cpdcrime2014_data['Primary Type'] == \"ROBBERY\"][\"Date\"].reset_index()\n",
    "\n",
    "#format date in dataframe to be mergeable with weather data \n",
    "robbery18['Date'] = pd.to_datetime(robbery18['Date']).dt.date\n",
    "robbery18['Date'] = pd.to_datetime(robbery18['Date'], infer_datetime_format=True)\n",
    "robbery17['Date'] = pd.to_datetime(robbery17['Date']).dt.date\n",
    "robbery17['Date'] = pd.to_datetime(robbery17['Date'], infer_datetime_format=True)\n",
    "robbery16['Date'] = pd.to_datetime(robbery16['Date']).dt.date\n",
    "robbery16['Date'] = pd.to_datetime(robbery16['Date'], infer_datetime_format=True)\n",
    "robbery15['Date'] = pd.to_datetime(robbery15['Date']).dt.date\n",
    "robbery15['Date'] = pd.to_datetime(robbery15['Date'], infer_datetime_format=True)\n",
    "robbery14['Date'] = pd.to_datetime(robbery14['Date']).dt.date\n",
    "robbery14['Date'] = pd.to_datetime(robbery14['Date'], infer_datetime_format=True)\n",
    "\n",
    "#sort date data for chronological order\n",
    "robbery18.sort_values(\"Date\", inplace=True)\n",
    "robbery17.sort_values(\"Date\", inplace=True)\n",
    "robbery16.sort_values(\"Date\", inplace=True)\n",
    "robbery15.sort_values(\"Date\", inplace=True)\n",
    "robbery14.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "#delete unnecessary index column\n",
    "del robbery18['index']\n",
    "del robbery17['index']\n",
    "del robbery16['index']\n",
    "del robbery15['index']\n",
    "del robbery14['index']\n",
    "\n",
    "#merge all reported robbery incidents for all five years\n",
    "robbery_all = robbery18.append([robbery17, robbery16, robbery15, robbery14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use to check output\n",
    "#robbery18\n",
    "#robbery18.dtypes\n",
    "#robbery17\n",
    "#robbery17.dtypes\n",
    "#robbery16\n",
    "#robbery16.dtypes\n",
    "#robbery15\n",
    "#robbery15.dtypes\n",
    "#robbery14\n",
    "#robbery14.dtypes\n",
    "#robbery_all\n",
    "#robbery_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9682</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9562</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9648</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9646</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9645</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9644</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9061</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9643</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9641</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9388</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9640</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9639</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9630</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9627</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9642</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9656</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9647</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9675</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9671</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9672</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9673</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9674</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9676</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9677</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9678</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9670</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9669</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9679</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9680</th>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9683 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date\n",
       "9682 2018-01-01\n",
       "9562 2018-01-01\n",
       "9648 2018-01-01\n",
       "40   2018-01-01\n",
       "9646 2018-01-01\n",
       "9645 2018-01-01\n",
       "9644 2018-01-01\n",
       "9061 2018-01-01\n",
       "9643 2018-01-01\n",
       "9641 2018-01-01\n",
       "9388 2018-01-01\n",
       "9640 2018-01-01\n",
       "9639 2018-01-01\n",
       "9630 2018-01-01\n",
       "9627 2018-01-01\n",
       "9642 2018-01-01\n",
       "9656 2018-01-01\n",
       "9647 2018-01-01\n",
       "9675 2018-01-01\n",
       "9671 2018-01-01\n",
       "9672 2018-01-01\n",
       "9673 2018-01-01\n",
       "9674 2018-01-01\n",
       "9676 2018-01-01\n",
       "9677 2018-01-01\n",
       "9678 2018-01-01\n",
       "9670 2018-01-01\n",
       "9669 2018-01-01\n",
       "9679 2018-01-01\n",
       "9680 2018-01-01\n",
       "...         ...\n",
       "170  2018-12-31\n",
       "163  2018-12-31\n",
       "164  2018-12-31\n",
       "165  2018-12-31\n",
       "166  2018-12-31\n",
       "167  2018-12-31\n",
       "161  2018-12-31\n",
       "160  2018-12-31\n",
       "122  2018-12-31\n",
       "121  2018-12-31\n",
       "120  2018-12-31\n",
       "195  2018-12-31\n",
       "50   2018-12-31\n",
       "198  2018-12-31\n",
       "199  2018-12-31\n",
       "200  2018-12-31\n",
       "201  2018-12-31\n",
       "202  2018-12-31\n",
       "151  2018-12-31\n",
       "203  2018-12-31\n",
       "109  2018-12-31\n",
       "205  2018-12-31\n",
       "206  2018-12-31\n",
       "210  2018-12-31\n",
       "179  2018-12-31\n",
       "177  2018-12-31\n",
       "176  2018-12-31\n",
       "149  2018-12-31\n",
       "204  2018-12-31\n",
       "209  2018-12-31\n",
       "\n",
       "[9683 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_robbery18 = robbery18\n",
    "temp_robbery18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupdate_robbery18 = robbery18\n",
    "groupdate_robbery17 = robbery17\n",
    "groupdate_robbery16 = robbery16\n",
    "groupdate_robbery15 = robbery15\n",
    "groupdate_robbery14 = robbery14\n",
    "\n",
    "#create number of reported incidents by date \n",
    "groupdate_robbery18['Reported Incidents'] = groupdate_robbery18['Date'].map(groupdate_robbery18['Date'].value_counts())\n",
    "groupdate_robbery17['Reported Incidents'] = groupdate_robbery17['Date'].map(groupdate_robbery17['Date'].value_counts())\n",
    "groupdate_robbery16['Reported Incidents'] = groupdate_robbery16['Date'].map(groupdate_robbery16['Date'].value_counts())\n",
    "groupdate_robbery15['Reported Incidents'] = groupdate_robbery15['Date'].map(groupdate_robbery15['Date'].value_counts())\n",
    "groupdate_robbery14['Reported Incidents'] = groupdate_robbery14['Date'].map(groupdate_robbery14['Date'].value_counts())\n",
    "\n",
    "#set index to date and drop duplicate data\n",
    "groupdate_robbery18.set_index('Date')\n",
    "groupdate_robbery18.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "groupdate_robbery17.set_index('Date')\n",
    "groupdate_robbery17.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "groupdate_robbery16.set_index('Date')\n",
    "groupdate_robbery16.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "groupdate_robbery15.set_index('Date')\n",
    "groupdate_robbery15.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n",
    "groupdate_robbery14.set_index('Date')\n",
    "groupdate_robbery14.drop_duplicates(subset =\"Date\", keep = 'first', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begin OpenWeatherMap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty lists\n",
    "Datetime = []\n",
    "Max_Temp = []\n",
    "Humidity = []\n",
    "Cloudiness = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json file path and reader\n",
    "file = open(\"weather_data.json\")\n",
    "json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop to grab the appropriate handles within .json file and append values to correlating empty lists\n",
    "for each in json_data[\"values\"]:\n",
    "        Datetime.append(each[\"dt\"])\n",
    "        Max_Temp.append(each[\"main\"][\"temp_max\"])\n",
    "        Humidity.append(each[\"main\"][\"humidity\"])\n",
    "        Cloudiness.append(each[\"clouds\"][\"all\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe from updated lists\n",
    "weather_df1 = pd.DataFrame({\"Datetime\": Datetime,\n",
    "                            \"Max_Temp\": Max_Temp,\n",
    "                            \"Humidity\": Humidity,\n",
    "                            \"Cloudiness\": Cloudiness})\n",
    "\n",
    "weather_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting unix timestamp to readable datetime\n",
    "weather_df1[\"Datetime\"]=pd.to_datetime(weather_df1[\"Datetime\"],unit='s')\n",
    "weather_df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new date column from datetime.\n",
    "weather_df1[\"Date\"] = pd.to_datetime(weather_df1['Datetime']).dt.date\n",
    "weather_df1[\"Date\"] = pd.to_datetime(weather_df1['Date'], infer_datetime_format=True)\n",
    "\n",
    "#from the newly created date column, splitting values into new month, day, and year columns.\n",
    "weather_df1['Month'] = weather_df1['Date'].dt.month\n",
    "weather_df1['Day'] = weather_df1['Date'].dt.day\n",
    "weather_df1['Year'] = weather_df1['Date'].dt.year\n",
    "\n",
    "weather_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting Kelvin to Fahrenheit and putting values into new column\n",
    "weather_df1[\"Max_Temp_(F)\"]=pyt.k2f(weather_df1[\"Max_Temp\"])\n",
    "weather_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete old Max_Temp and Datetime columns\n",
    "del weather_df1[\"Max_Temp\"]\n",
    "del weather_df1[\"Datetime\"]\n",
    "weather_df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping by date and calling the mean of other columns\n",
    "weather_df2 = pd.DataFrame(weather_df1.groupby(\"Date\")[\"Humidity\", \"Cloudiness\", \"Max_Temp_(F)\", \"Month\", \"Day\", \"Year\"].mean())\n",
    "weather_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime('2014-10-14')\n",
    "weather_df2.loc[pd.to_datetime('2014-07-26')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begin merging CPD crime data with weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine weather data into crime incident data for final dataframes\n",
    "weatherbattery18 = battery18.merge(weather_df2, how='left', on='Date')\n",
    "weatherbattery17 = battery17.merge(weather_df2, how='left', on='Date')\n",
    "weatherbattery16 = battery16.merge(weather_df2, how='left', on='Date')\n",
    "weatherbattery15 = battery15.merge(weather_df2, how='left', on='Date')\n",
    "weatherbattery14 = battery14.merge(weather_df2, how='left', on='Date')\n",
    "weatherbatteryall = battery_all.merge(weather_df2, how='left', on='Date')\n",
    "\n",
    "#calculate number of NaN values then drop them\n",
    "weatherbattery18nan = pd.isnull(weatherbattery18).sum()\n",
    "weatherbattery18 = weatherbattery18.dropna(how='any')\n",
    "weatherbattery17nan = pd.isnull(weatherbattery17).sum()\n",
    "weatherbattery17 = weatherbattery17.dropna(how='any')\n",
    "weatherbattery16nan = pd.isnull(weatherbattery16).sum()\n",
    "weatherbattery16 = weatherbattery16.dropna(how='any')\n",
    "weatherbattery15nan = pd.isnull(weatherbattery15).sum()\n",
    "weatherbattery15 = weatherbattery15.dropna(how='any')\n",
    "weatherbattery14nan = pd.isnull(weatherbattery14).sum()\n",
    "weatherbattery14 = weatherbattery14.dropna(how='any')\n",
    "weatherbatteryallnan = pd.isnull(weatherbatteryall).sum()\n",
    "weatherbatteryall = weatherbatteryall.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use to check outputs\n",
    "#weatherbattery18nan\n",
    "#weatherbattery18\n",
    "#weatherbattery17nan\n",
    "#weatherbattery17\n",
    "#weatherbattery16nan\n",
    "#weatherbattery16\n",
    "#weatherbattery15nan\n",
    "#weatherbattery15\n",
    "#weatherbattery14nan\n",
    "#weatherbattery14\n",
    "#weatherbatteryallnan\n",
    "#weatherbatteryall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine weather data into crime incident data for final dataframes\n",
    "weathercsa18 = csa18.merge(weather_df2, how='left', on='Date')\n",
    "weathercsa17 = csa17.merge(weather_df2, how='left', on='Date')\n",
    "weathercsa16 = csa16.merge(weather_df2, how='left', on='Date')\n",
    "weathercsa15 = csa15.merge(weather_df2, how='left', on='Date')\n",
    "weathercsa14 = csa14.merge(weather_df2, how='left', on='Date')\n",
    "weathercsaall = csa_all.merge(weather_df2, how='left', on='Date')\n",
    "\n",
    "#calculate number of NaN values then drop them\n",
    "weathercsa18nan = pd.isnull(weathercsa18).sum()\n",
    "weathercsa18 = weathercsa18.dropna(how='any')\n",
    "weathercsa17nan = pd.isnull(weathercsa17).sum()\n",
    "weathercsa17 = weathercsa17.dropna(how='any')\n",
    "weathercsa16nan = pd.isnull(weathercsa16).sum()\n",
    "weathercsa16 = weathercsa16.dropna(how='any')\n",
    "weathercsa15nan = pd.isnull(weathercsa15).sum()\n",
    "weathercsa15 = weathercsa15.dropna(how='any')\n",
    "weathercsa14nan = pd.isnull(weathercsa14).sum()\n",
    "weathercsa14 = weathercsa14.dropna(how='any')\n",
    "weathercsaallnan = pd.isnull(weathercsaall).sum()\n",
    "weathercsaall = weathercsaall.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use to check outputs\n",
    "#weathercsa18nan\n",
    "#weathercsa18\n",
    "#weathercsa17nan\n",
    "#weathercsa17\n",
    "#weathercsa16nan\n",
    "#weathercsa16\n",
    "#weathercsa15nan\n",
    "#weathercsa15\n",
    "#weathercsa14nan\n",
    "#weathercsa14\n",
    "#weathercsaallnan\n",
    "#weathercsaall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine weather data into crime incident data for final dataframes\n",
    "weatherhomicide18 = homicide18.merge(weather_df2, how='left', on='Date')\n",
    "weatherhomicide17 = homicide17.merge(weather_df2, how='left', on='Date')\n",
    "weatherhomicide16 = homicide16.merge(weather_df2, how='left', on='Date')\n",
    "weatherhomicide15 = homicide15.merge(weather_df2, how='left', on='Date')\n",
    "weatherhomicide14 = homicide14.merge(weather_df2, how='left', on='Date')\n",
    "weatherhomicideall = homicide_all.merge(weather_df2, how='left', on='Date')\n",
    "\n",
    "#calculate number of NaN values then drop them\n",
    "weatherhomicide18nan = pd.isnull(weatherhomicide18).sum()\n",
    "weatherhomicide18 = weatherhomicide18.dropna(how='any')\n",
    "weatherhomicide17nan = pd.isnull(weatherhomicide17).sum()\n",
    "weatherhomicide17 = weatherhomicide17.dropna(how='any')\n",
    "weatherhomicide16nan = pd.isnull(weatherhomicide16).sum()\n",
    "weatherhomicide16 = weatherhomicide16.dropna(how='any')\n",
    "weatherhomicide15nan = pd.isnull(weatherhomicide15).sum()\n",
    "weatherhomicide15 = weatherhomicide15.dropna(how='any')\n",
    "weatherhomicide14nan = pd.isnull(weatherhomicide14).sum()\n",
    "weatherhomicide14 = weatherhomicide14.dropna(how='any')\n",
    "weatherhomicideallnan = pd.isnull(weatherhomicideall).sum()\n",
    "weatherhomicideall = weatherhomicideall.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weatherhomicide14' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-88d0a06ad436>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#weatherhomicide15\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#weatherhomicide14nan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mweatherhomicide14\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m#weatherhomicideallnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#weatherhomicideall\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weatherhomicide14' is not defined"
     ]
    }
   ],
   "source": [
    "#use to check outputs\n",
    "#weatherhomicide18nan\n",
    "#weatherhomicide18\n",
    "#weatherhomicide17nan\n",
    "#weatherhomicide17\n",
    "#weatherhomicide16nan\n",
    "#weatherhomicide16\n",
    "#weatherhomicide15nan\n",
    "#weatherhomicide15\n",
    "#weatherhomicide14nan\n",
    "weatherhomicide14\n",
    "#weatherhomicideallnan\n",
    "#weatherhomicideall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine weather data into crime incident data for final dataframes\n",
    "weatherrobbery18 = robbery18.merge(weather_df2, how='left', on='Date')\n",
    "weatherrobbery17 = robbery17.merge(weather_df2, how='left', on='Date')\n",
    "weatherrobbery16 = robbery16.merge(weather_df2, how='left', on='Date')\n",
    "weatherrobbery15 = robbery15.merge(weather_df2, how='left', on='Date')\n",
    "weatherrobbery14 = robbery14.merge(weather_df2, how='left', on='Date')\n",
    "weatherrobberyall = robbery_all.merge(weather_df2, how='left', on='Date')\n",
    "\n",
    "#calculate number of NaN values then drop them\n",
    "weatherrobbery18nan = pd.isnull(weatherrobbery18).sum()\n",
    "weatherrobbery18 = weatherrobbery18.dropna(how='any')\n",
    "weatherrobbery17nan = pd.isnull(weatherrobbery17).sum()\n",
    "weatherrobbery17 = weatherrobbery17.dropna(how='any')\n",
    "weatherrobbery16nan = pd.isnull(weatherrobbery16).sum()\n",
    "weatherrobbery16 = weatherrobbery16.dropna(how='any')\n",
    "weatherrobbery15nan = pd.isnull(weatherrobbery15).sum()\n",
    "weatherrobbery15 = weatherrobbery15.dropna(how='any')\n",
    "weatherrobbery14nan = pd.isnull(weatherrobbery14).sum()\n",
    "weatherrobbery14 = weatherrobbery14.dropna(how='any')\n",
    "weatherrobberyallnan = pd.isnull(weatherrobberyall).sum()\n",
    "weatherrobberyall = weatherrobberyall.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use to check outputs\n",
    "#weatherrobbery18nan\n",
    "#weatherrobbery18\n",
    "#weatherrobbery17nan\n",
    "#weatherrobbery17\n",
    "#weatherrobbery16nan\n",
    "#weatherrobbery16\n",
    "#weatherrobbery15nan\n",
    "#weatherrobbery15\n",
    "#weatherrobbery14nan\n",
    "#weatherrobbery14\n",
    "#weatherrobberyall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temperature vs time\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "for y in weatherbatteryall['Year'].unique().tolist():\n",
    "   dat = weatherbatteryall[weatherbatteryall['Year']==y]\n",
    "   dat = dat[['Year','Month','Max_Temp_(F)']].drop_duplicates()\n",
    "   dat = pd.DataFrame(dat.groupby(['Year','Month'])['Max_Temp_(F)'].mean()) \n",
    "   temps = [x[0] for x in dat.values.tolist()]\n",
    "   dates = dat.index.levels[1].tolist() \n",
    "   ax.plot(dates,temps,label=y)\n",
    "plt.xticks(weatherbatteryall[weatherbatteryall['Year']==2018]['Month'].unique().tolist())\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel(\"Average Temperature(F)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
